# --------------------------------------------------------------------------
# Script: solve_from_partitions.py
# Description: Loads pre-generated partitions and solves them using specified methods.
# --------------------------------------------------------------------------

# 0. Imports
# --------------------------------------------------------------------------
import os
import time
import argparse
import logging
import csv
import torch
import numpy as np
import pickle
import json
import math # Ensure math is imported
import gc # For memory release
import concurrent.futures # For parallel execution
from functools import partial
import importlib # Ensure importlib is imported
from tqdm import tqdm # Import tqdm

# Project-specific imports
from utils import (
    load_dataset, get_env, VRP_DATA_FORMAT, DATASET_PATHS, 
    AverageMeter, TimeEstimator, seed_everything
)
from partitioner_solver_utils import (
    load_moe_model, create_subproblem_instance, 
    pad_subproblem_batch, prepare_batch_tensor_data, 
    solve_vrp_batch, DEFAULT_MODEL_PARAMS
)
# ortools_solve_vrp will be loaded dynamically by tasks or if ortools_available

# --------------------------------------------------------------------------
# 1. Constants & Configuration
# --------------------------------------------------------------------------
PROBLEM_SIZES_TO_TEST = [50, 100, 200, 500, 1000, 2000]
SUPPORTED_PROBLEM_TYPES = ["CVRP", "OVRP", "VRPB", "VRPL", "VRPTW"]
DEFAULT_NUM_INSTANCES = 50 # Default number of original instances to process from partition data
DEFAULT_INPUT_DIR = 'partition_results' 
DEFAULT_PARTITION_RUN_ID = None # User should specify or we can try to find the latest

# Fixed list of 8 methods this script will specifically support
ALL_METHODS = [
    'DirectSolver',
    'DirectORTools',
    'PartitionSolver_m1',
    'PartitionSolver_m3',
    'PartitionSolver_adaptive',
    'PartitionORTools_m1',
    'PartitionORTools_m3',
    'PartitionORTools_adaptive',
]

# Mapping for partition-based methods to their corresponding .pkl file names
# These filenames are assumed to be generated by generate_partitions.py under each instance's folder.
PARTITION_SOLVER_METHOD_TO_FILE = {
    'PartitionSolver_m1': "merged_nodes_m1.pkl",
    'PartitionSolver_m3': "merged_nodes_m3.pkl",
    'PartitionSolver_adaptive': "merged_nodes_adaptive.pkl",
}

PARTITION_ORTOOLS_METHOD_TO_FILE = {
    'PartitionORTools_m1': "merged_nodes_m1.pkl", # Uses same partition files
    'PartitionORTools_m3': "merged_nodes_m3.pkl",
    'PartitionORTools_adaptive': "merged_nodes_adaptive.pkl",
}

# Method Groupings (Subset of ALL_METHODS)
ALL_SOLVER_METHODS = ['DirectSolver', 'PartitionSolver_m1', 'PartitionSolver_m3', 'PartitionSolver_adaptive']
ALL_ORTOOLS_METHODS = ['DirectORTools', 'PartitionORTools_m1', 'PartitionORTools_m3', 'PartitionORTools_adaptive']

ALL_PARTITION_SOLVER_METHODS = [m for m in ALL_METHODS if 'PartitionSolver' in m]
ALL_PARTITION_ORTOOLS_METHODS = [m for m in ALL_METHODS if 'PartitionORTools' in m]
ALL_PARTITION_METHODS = ALL_PARTITION_SOLVER_METHODS + ALL_PARTITION_ORTOOLS_METHODS

ALL_M1_METHODS = [m for m in ALL_METHODS if '_m1' in m]
ALL_M3_METHODS = [m for m in ALL_METHODS if '_m3' in m]
ALL_ADAPTIVE_METHODS = [m for m in ALL_METHODS if '_adaptive' in m]

ORTOOLS_TIMELIMIT_MAP = { # For subproblems or direct OR-Tools solving
    50: 2,
    100: 2,
    200: 5,
    500: 10,
    1000: 20,
    2000: 40,
    5000: 60,
}

# --------------------------------------------------------------------------
# 2. Argument Parser
# --------------------------------------------------------------------------
def parse_arguments_solver():
    parser = argparse.ArgumentParser(description="VRP Solver Framework for Pre-Partitioned Instances")
    
    # --- Input Partition Data ---
    parser.add_argument('--input_dir', type=str, default=DEFAULT_INPUT_DIR,
                        help=f"Root directory where partition results are stored. Default: {DEFAULT_INPUT_DIR}")
    parser.add_argument('--partition_run_id', type=str, default=DEFAULT_PARTITION_RUN_ID, required=True,
                        help="The specific run ID (e.g., PartitionGen_YYYYMMDD_HHMMSS) to load partitions from. Required.")

    # --- Test Configuration (Problem, Size, Instance Count to load) ---
    parser.add_argument('--problems', nargs='+', default=["CVRP"], 
                        help=f"List of VRP problem types to test from the partition data, or 'ALL' to run all supported types: {SUPPORTED_PROBLEM_TYPES}")
    parser.add_argument('--sizes', nargs='+', type=int, default=PROBLEM_SIZES_TO_TEST, 
                        help="List of problem sizes (N) to test from the partition data.")
    parser.add_argument('--num_instances', type=int, default=DEFAULT_NUM_INSTANCES, 
                        help="Number of original instances to process per problem size/type from the partition data.")
    parser.add_argument('--seed', type=int, default=2024, help="Random seed.")

    # --- Methods to Run (from the fixed set of 8) ---
    parser.add_argument('--methods', type=str, nargs='+', default=ALL_METHODS, 
                        help="Method(s) to run. Choises are from the fixed set of 8 methods. "
                             "Can also use groups: AllSolver, AllORTools, AllPartitionSolver, AllPartitionORTools, AllPartition, All_m1, All_m3, All_adaptive, ALL.")
    
    # # --- Batch Size for NN Solver (Direct or Subproblems) ---
    # parser.add_argument('--test_batch_size', type=int, default=16, 
    #                     help="Batch size for processing instances/subproblems with the NN Solver.")

    # --- Solver Model Path (Required for *Solver methods) ---
    parser.add_argument('--solver_checkpoint', type=str, default=None, 
                        help="Path to the pre-trained Solver model checkpoint. Required for *Solver methods.")

    # --- Solver Model Parameters (Copied from test_comparison.py) --- 
    parser.add_argument('--solver_model_type', type=str, default="MOE", help="Solver model architecture type.")
    parser.add_argument('--solver_num_experts', type=int, default=8, help="Number of experts for Solver model (if MOE).")
    parser.add_argument('--solver_embedding_dim', type=int, default=128, help="Embedding dimension for Solver model.")
    parser.add_argument('--solver_ff_hidden_dim', type=int, default=512, help="Feed-forward hidden dim for Solver model.")
    parser.add_argument('--solver_encoder_layer_num', type=int, default=6, help="Number of encoder layers for Solver model.")
    
    # --- Method Specific Parameters ---
    parser.add_argument('--solver_aug_factor', type=int, default=1, help="Augmentation factor for Solver model inference.")
    parser.add_argument('--ortools_stagnation_duration', type=int, default=10, 
                        help="OR-Tools: Stagnation duration (seconds). Default: 10s.")
    parser.add_argument('--ortools_min_improvement_pct', type=float, default=0.5, 
                        help="OR-Tools: Min improvement percentage for stagnation. Default: 0.5 (0.5%%).")

    # --- Execution Control ---
    # 'workers' is for instance-level parallelism (which user wants to avoid for now)
    # OR for DirectORTools if processing a batch of original instances.
    # For PartitionORTools, subproblem parallelism is handled by subproblem_workers.
    # parser.add_argument('--workers', type=int, default=1,
    #                     help="Max workers for parallel OR-Tools (DirectORTools instance-level, not used by default for PartitionORTools instance-level). Default: 1 (sequential).")
    parser.add_argument('--subproblem_workers', type=int, default=16, 
                        help="Max workers for OR-Tools when solving subproblems within PartitionORTools methods. Default: 4. Set to 0 for os.cpu_count().")
    parser.add_argument('--no_cuda', action='store_true', help='Disable CUDA')
    parser.add_argument('--gpu_id', type=int, default=0, help="GPU ID to use if CUDA is enabled.")

    # --- Output ---
    parser.add_argument('--output_csv', type=str, default='solve_from_partitions_results.csv', 
                        help="Path to save the CSV results.")
    parser.add_argument('--log_file', type=str, default='solve_from_partitions.log', 
                        help="Path to save the log file.")
    parser.add_argument('--verbose_log', action='store_true', help="Print detailed logs (INFO level) to console.")

    args = parser.parse_args()
    return args

# --------------------------------------------------------------------------
# 3. Logging Setup (Placeholder - will copy from test_comparison.py later)
# --------------------------------------------------------------------------
def setup_logging(log_file_path, verbose=False):
    """ Configures logging to file (INFO) and console (WARNING or INFO). """
    log_formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s')
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO) # Set root logger level

    # Clear existing handlers (important if run multiple times in notebooks etc.)
    root_logger.handlers.clear()

    # File Handler (always INFO)
    try:
        file_handler = logging.FileHandler(log_file_path, mode='w')
        file_handler.setFormatter(log_formatter)
        file_handler.setLevel(logging.INFO)
        root_logger.addHandler(file_handler)
    except Exception as e:
        print(f"Error setting up file logger at {log_file_path}: {e}")

    # Console Handler (level depends on verbosity)
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(log_formatter)
    if verbose:
        console_handler.setLevel(logging.INFO)
    else:
        console_handler.setLevel(logging.WARNING)
    root_logger.addHandler(console_handler)

# --------------------------------------------------------------------------
# 4. Method-Specific Helper Functions / Tasks (Placeholders)
# --------------------------------------------------------------------------

def run_direct_solver_single_instance(
    original_instance_tuple,
    problem_type,
    solver_model,
    solver_env_class,
    device,
    args_namespace # Pass the script's args for solver_aug_factor etc.
):
    """ Solves a single instance directly using the Solver model. """
    if not original_instance_tuple or not solver_model or not solver_env_class:
        logging.error("run_direct_solver_single_instance: Missing instance, model, or env class.")
        return {'score': float('inf'), 'total_time_seconds': 0, 'solve_time_seconds': 0, 'num_subproblems': 0}

    start_time = time.time()
    final_result = {'score': float('inf'), 'total_time_seconds': 0, 'solve_time_seconds': 0, 'num_subproblems': 0}

    try:
        batch_instance_tuples = [original_instance_tuple] # Batch of one
        
        # Determine padding size based on this single instance
        key_to_index = {key: idx for idx, key in enumerate(VRP_DATA_FORMAT.get(problem_type, []))}
        node_xy_idx = key_to_index.get('node_xy')
        if node_xy_idx is None: raise ValueError("Cannot find 'node_xy' index in VRP_DATA_FORMAT")
        
        num_nodes_in_instance = len(original_instance_tuple[node_xy_idx])
        target_pad_size = max(1, num_nodes_in_instance) 

        padded_batch_tuples, actual_pad_size = pad_subproblem_batch(batch_instance_tuples, problem_type, target_pad_size)
        if not padded_batch_tuples or len(padded_batch_tuples) != 1 or actual_pad_size != target_pad_size:
             raise ValueError("Failed to pad instance for direct solver.")

        # Prepare tensor data
        padded_batch_tensor_data = prepare_batch_tensor_data(
            padded_batch_tuples, problem_type, device
        )
        if not padded_batch_tensor_data:
             raise ValueError("Failed to prepare tensor data for direct solver.")

        # Solve using the solver model
        solver_model.eval() 
        solver_results = solve_vrp_batch(
            solver_model=solver_model,
            solver_env_class=solver_env_class,
            original_instance_tuples=batch_instance_tuples, 
            padded_batch_data=padded_batch_tensor_data,
            padded_problem_size=actual_pad_size, 
            problem_type=problem_type,
            device=device,
            aug_factor=args_namespace.solver_aug_factor
        )

        if not solver_results or len(solver_results) != 1:
            raise ValueError("Direct solver results count mismatch or empty.")
            
        score, _path = solver_results[0]
        solve_time = time.time() - start_time
        
        final_result['score'] = score if score != float('inf') and score is not None and not math.isnan(score) else float('inf')
        final_result['total_time_seconds'] = solve_time
        final_result['solve_time_seconds'] = solve_time
        # 'load_data_time_seconds' will be handled outside, related to partition file loading for other methods
        # For direct methods, it's part of the main instance loading time.
            
    except Exception as e:
        logging.error(f"Error in run_direct_solver_single_instance: {e}", exc_info=True)
        solve_time = time.time() - start_time # Recalculate time even on error
        final_result['score'] = float('inf')
        final_result['total_time_seconds'] = solve_time 
        final_result['solve_time_seconds'] = solve_time

    return final_result

# Adapted from test_comparison.py for use here
# ortools_solve_vrp_func is expected to be loaded globally or passed if this were more modular
# For now, it will try to import directly within the function if not available globally.
def run_direct_ortools_single_instance(
    instance_tuple,
    problem_type,
    timelimit, 
    args_namespace, # Pass args for stagnation control
    ortools_solve_vrp_function # Pass the loaded function
):
    """ Solves a single instance directly using OR-Tools. """
    start_time = time.time()
    cost, path = float('inf'), []
    
    if ortools_solve_vrp_function:
        try:
            cost, path = ortools_solve_vrp_function(
            instance_tuple, 
            problem_type, 
            timelimit, 
            stagnation_duration=args_namespace.ortools_stagnation_duration, 
            min_stagnation_improvement_pct=args_namespace.ortools_min_improvement_pct
        )
        except Exception as e:
            logging.error(f"Error running OR-Tools directly on instance: {e}", exc_info=True)
            cost, path = float('inf'), []
    else:
        logging.error("OR-Tools solve function not provided to run_direct_ortools_single_instance.")
        cost, path = float('inf'), []
    
    solve_time = time.time() - start_time

    return {
        'score': cost if cost != float('inf') and cost is not None and not math.isnan(cost) else float('inf'),
        'total_time_seconds': solve_time,
        'solve_time_seconds': solve_time,
        'num_subproblems': 0 
        # 'load_data_time_seconds' is not applicable here in the same way as partition methods
    }

def run_partition_solver_from_loaded_partitions(
    original_instance_tuple,
    problem_type, 
    subproblem_node_lists, # Loaded from .pkl file
    solver_model, 
    solver_env_class, 
    device, 
    args_namespace # For solver_aug_factor
):
    """ 
    Solves an original instance by first creating subproblem instances from pre-loaded 
    node lists, then solving these subproblems using the NN Solver.
    """
    if not original_instance_tuple or not solver_model or not solver_env_class:
        logging.error("run_partition_solver: Missing original instance, model, or env class.")
        return {'score': float('inf'), 'solve_time_seconds': 0, 'num_subproblems': 0, 'error': 'MissingInput'}
    
    if not subproblem_node_lists:
        logging.warning(f"run_partition_solver: Received empty subproblem_node_lists for {problem_type}. Returning inf score.")
        return {'score': float('inf'), 'solve_time_seconds': 0, 'num_subproblems': 0, 'error': 'EmptyNodeLists'}

    solve_time_start = time.time() # Timer for the solving part only
    
    # 1. Create subproblem instance tuples
    subproblem_instance_tuples = []
    num_original_nodes = 0
    try: # Get num_original_nodes for validation in create_subproblem_instance
        node_xy_idx_orig = VRP_DATA_FORMAT[problem_type].index('node_xy')
        num_original_nodes = len(original_instance_tuple[node_xy_idx_orig])
    except (KeyError, IndexError, TypeError) as e:
        logging.error(f"run_partition_solver: Error accessing original node_xy for count: {e}")
        return {'score': float('inf'), 'solve_time_seconds': 0, 'num_subproblems': 0, 'error': 'OriginalNodeAccessError'}

    for node_indices_1based_list in subproblem_node_lists:
        if not node_indices_1based_list: # Skip empty lists from pkl
            logging.debug("Skipping empty node_indices_1based_list in subproblem_node_lists.")
            continue
        # Validate indices before creating subproblem (create_subproblem_instance also does this)
        valid_indices_for_sub = [idx for idx in node_indices_1based_list if 1 <= idx <= num_original_nodes]
        if not valid_indices_for_sub:
            logging.warning(f"A subproblem node list {node_indices_1based_list} contained no valid nodes. Skipping.")
            continue
        
        sub_instance = create_subproblem_instance(original_instance_tuple, problem_type, valid_indices_for_sub)
        if sub_instance:
            subproblem_instance_tuples.append(sub_instance)
        else:
            logging.warning(f"Failed to create subproblem instance for node list: {valid_indices_for_sub}. Skipping this subproblem.")

    if not subproblem_instance_tuples:
        logging.warning(f"No valid subproblem instances could be created from the loaded node lists for {problem_type}. Returning inf score.")
        # This can happen if all subproblem_node_lists were empty or invalid
        return {'score': float('inf'), 'solve_time_seconds': time.time() - solve_time_start, 'num_subproblems': 0, 'error': 'NoValidSubproblemsCreated'}
    
    num_actual_subproblems = len(subproblem_instance_tuples)

    # 2. Pad the batch of subproblems
    # Determine padding size based on the max nodes in *this batch* of subproblems
    max_nodes_in_sub_batch = 0
    key_to_index_sub = {key: idx for idx, key in enumerate(VRP_DATA_FORMAT.get(problem_type, []))}
    node_xy_idx_sub = key_to_index_sub.get('node_xy')
    if node_xy_idx_sub is None: 
        logging.error("Cannot find 'node_xy' index for subproblem padding.")
        return {'score': float('inf'), 'solve_time_seconds': time.time() - solve_time_start, 'num_subproblems': num_actual_subproblems, 'error': 'SubproblemNodeXYMissing'}
    
    for sub_inst_tuple in subproblem_instance_tuples:
        max_nodes_in_sub_batch = max(max_nodes_in_sub_batch, len(sub_inst_tuple[node_xy_idx_sub]))
    
    target_pad_size_for_subs = max(1, max_nodes_in_sub_batch)
    
    padded_subproblem_batch, actual_pad_size_for_subs = pad_subproblem_batch(
        subproblem_instance_tuples, problem_type, target_pad_size_for_subs
    )
    if not padded_subproblem_batch or len(padded_subproblem_batch) != num_actual_subproblems:
        logging.error("Failed to pad subproblem batch correctly.")
        return {'score': float('inf'), 'solve_time_seconds': time.time() - solve_time_start, 'num_subproblems': num_actual_subproblems, 'error': 'SubproblemPaddingError'}

    # 3. Prepare tensor data
    padded_subproblem_tensor_data = prepare_batch_tensor_data(
        padded_subproblem_batch, problem_type, device
    )
    if not padded_subproblem_tensor_data:
        logging.error("Failed to prepare tensor data for subproblem batch.")
        return {'score': float('inf'), 'solve_time_seconds': time.time() - solve_time_start, 'num_subproblems': num_actual_subproblems, 'error': 'SubproblemTensorPrepError'}

    # 4. Solve the batch of subproblems
    solver_model.eval()
    flat_solver_results = solve_vrp_batch(
        solver_model=solver_model,
        solver_env_class=solver_env_class,
        original_instance_tuples=subproblem_instance_tuples, # Pass the unpadded subproblem tuples
        padded_batch_data=padded_subproblem_tensor_data,
        padded_problem_size=actual_pad_size_for_subs,
        problem_type=problem_type,
        device=device,
        aug_factor=args_namespace.solver_aug_factor
    )

    if not flat_solver_results or len(flat_solver_results) != num_actual_subproblems:
        logging.error(f"Subproblem solver results count mismatch. Expected {num_actual_subproblems}, got {len(flat_solver_results) if flat_solver_results else 0}.")
        # Populate with failures if mismatch
        error_results_fill = [(float('inf'), None)] * (num_actual_subproblems - (len(flat_solver_results) if flat_solver_results else 0))
        flat_solver_results = (flat_solver_results if flat_solver_results else []) + error_results_fill

    # 5. Aggregate scores
    total_aggregated_score = 0
    all_subproblems_solved_successfully = True
    for sub_score, _sub_path in flat_solver_results:
        if sub_score == float('inf') or sub_score is None or math.isnan(sub_score):
            total_aggregated_score = float('inf')
            all_subproblems_solved_successfully = False
            logging.warning(f"A subproblem failed to solve (score: {sub_score}). Original instance score will be inf.")
            break
        total_aggregated_score += sub_score
    
    current_solve_time = time.time() - solve_time_start

    return {
        'score': total_aggregated_score,
        'solve_time_seconds': current_solve_time,
        'num_subproblems': num_actual_subproblems
    }

# Helper for ProcessPoolExecutor for PartitionORTools
def _ortools_subproblem_task_runner(params_tuple):
    """
    Unpacks arguments and calls the OR-Tools solver for a single subproblem.
    Expected params_tuple: (subproblem_instance_tuple, problem_type, timelimit_for_subproblem, 
                              args_namespace_for_stagnation, ortools_solve_vrp_function_to_call)
    Returns: (cost, path_list) tuple from ortools_solve_vrp_function_to_call
    """
    sub_instance, prob_type, sub_timelimit, stagnation_args, ortools_func = params_tuple
    if not ortools_func:
        # This should not happen if ortools_available was checked before calling a PartitionORTools method.
        # However, as a safeguard within the process worker:
        logging.error("_ortools_subproblem_task_runner: OR-Tools function not provided.")
        return float('inf'), []
    try:
        # Call the OR-Tools solver for the subproblem
        cost, path = ortools_func(
            sub_instance, 
            prob_type, 
            sub_timelimit,
            stagnation_duration=stagnation_args.ortools_stagnation_duration, 
            min_stagnation_improvement_pct=stagnation_args.ortools_min_improvement_pct
        )
        return cost, path
    except Exception as e:
        logging.error(f"Error in OR-Tools subproblem task: {e}", exc_info=True)
        return float('inf'), [] # Return failure state

def run_partition_ortools_from_loaded_partitions(
    original_instance_tuple,
    problem_type,
    subproblem_node_lists, # Loaded from .pkl file
    effective_timelimit_for_subproblems, # Overall timelimit for this size, used per subproblem here
    args_namespace, # For subproblem_workers and OR-Tools stagnation parameters
    ortools_solve_vrp_function # The actual OR-Tools solve function
):
    """
    Solves an original instance by creating subproblems from loaded node lists, 
    then solving these subproblems in parallel using OR-Tools.
    """
    if not original_instance_tuple or not ortools_solve_vrp_function:
        logging.error("run_partition_ortools: Missing original instance or OR-Tools function.")
        return {'score': float('inf'), 'solve_time_seconds': 0, 'num_subproblems': 0, 'error': 'MissingInputOrORToolsFunc'}

    if not subproblem_node_lists:
        logging.warning(f"run_partition_ortools: Received empty subproblem_node_lists for {problem_type}. Returning inf score.")
        return {'score': float('inf'), 'solve_time_seconds': 0, 'num_subproblems': 0, 'error': 'EmptyNodeLists'}

    solve_time_start = time.time()

    # 1. Create subproblem instance tuples
    subproblem_instance_tuples = []
    num_original_nodes = 0
    try: 
        node_xy_idx_orig = VRP_DATA_FORMAT[problem_type].index('node_xy')
        num_original_nodes = len(original_instance_tuple[node_xy_idx_orig])
    except (KeyError, IndexError, TypeError) as e:
        logging.error(f"run_partition_ortools: Error accessing original node_xy for count: {e}")
        return {'score': float('inf'), 'solve_time_seconds': 0, 'num_subproblems': 0, 'error': 'OriginalNodeAccessError'}

    for node_indices_1based_list in subproblem_node_lists:
        if not node_indices_1based_list: continue
        valid_indices_for_sub = [idx for idx in node_indices_1based_list if 1 <= idx <= num_original_nodes]
        if not valid_indices_for_sub: continue
        sub_instance = create_subproblem_instance(original_instance_tuple, problem_type, valid_indices_for_sub)
        if sub_instance: subproblem_instance_tuples.append(sub_instance)

    if not subproblem_instance_tuples:
        logging.warning(f"No valid subproblem instances created from loaded node lists for {problem_type} (PartitionORTools). Score inf.")
        return {'score': float('inf'), 'solve_time_seconds': time.time() - solve_time_start, 'num_subproblems': 0, 'error': 'NoValidSubproblemsCreated'}
    
    num_actual_subproblems = len(subproblem_instance_tuples)

    # 2. Parallel OR-Tools Solving of subproblems
    subproblem_tasks_for_pool = []
    for sub_inst_tuple in subproblem_instance_tuples:
        subproblem_tasks_for_pool.append((
            sub_inst_tuple, 
            problem_type, 
            effective_timelimit_for_subproblems, # Use the same timelimit for each subproblem
            args_namespace, # Pass the full args for stagnation params
            ortools_solve_vrp_function
        ))

    flat_ortools_results = [] # List to store (cost, path) tuples
    if subproblem_tasks_for_pool:
        num_sub_workers = args_namespace.subproblem_workers if args_namespace.subproblem_workers > 0 else os.cpu_count()
        num_sub_workers = min(num_sub_workers, len(subproblem_tasks_for_pool), os.cpu_count() or 1) # Cap workers
        logging.info(f"Solving {len(subproblem_tasks_for_pool)} subproblems using OR-Tools with {num_sub_workers} workers.")
        
        try:
            # Using ProcessPoolExecutor as OR-Tools Python wrapper might not be fully thread-safe for complex scenarios
            # or might have GIL issues that limit ThreadPoolExecutor benefits.
            with concurrent.futures.ProcessPoolExecutor(max_workers=num_sub_workers) as executor:
                results_iterator = executor.map(_ortools_subproblem_task_runner, subproblem_tasks_for_pool)
                flat_ortools_results = list(results_iterator) 
        except Exception as pool_exec_error:
            logging.error(f"Error during parallel OR-Tools execution (subproblems): {pool_exec_error}", exc_info=True)
            flat_ortools_results = [(float('inf'), [])] * len(subproblem_tasks_for_pool)

        if len(flat_ortools_results) != len(subproblem_tasks_for_pool):
            logging.error("Parallel OR-Tools subproblem result count mismatch!")
            needed = len(subproblem_tasks_for_pool) - len(flat_ortools_results)
            flat_ortools_results.extend([(float('inf'), [])] * needed)
    
    # 3. Aggregate scores
    total_aggregated_score = 0
    for sub_cost, _sub_path in flat_ortools_results:
        if sub_cost == float('inf') or sub_cost is None or math.isnan(sub_cost):
            total_aggregated_score = float('inf')
            logging.warning("An OR-Tools subproblem failed to solve. Original instance score will be inf.")
            break
        total_aggregated_score += sub_cost

    current_solve_time = time.time() - solve_time_start

    return {
        'score': total_aggregated_score,
        'solve_time_seconds': current_solve_time,
        'num_subproblems': num_actual_subproblems
    }

# --------------------------------------------------------------------------
# 5. Main Execution Logic (Placeholder)
# --------------------------------------------------------------------------
def main_solver():
    args = parse_arguments_solver()

    # --- Setup Output Paths --- 
    run_timestamp = time.strftime('%Y%m%d_%H%M%S')
    run_id = f"SolveRun_{args.partition_run_id}_{run_timestamp}" # Include partition run id for context
    # Base output dir for all solve_from_partitions runs, then specific run_id
    base_output_dir = os.path.join('results', 'solve_from_partitions_output', run_id)
    try:
        os.makedirs(base_output_dir, exist_ok=True)
    except OSError as e:
        print(f"Error creating output directory {base_output_dir}: {e}. Files will be saved in current directory.")
        base_output_dir = "." # Fallback to current directory
        
    # Construct full paths using the base directory and filenames from args
    full_log_path = os.path.join(base_output_dir, args.log_file) 
    full_csv_path = os.path.join(base_output_dir, args.output_csv)

    # --- Setup Logging with Full Path --- 
    setup_logging(full_log_path, args.verbose_log)
    seed_everything(args.seed)

    # --- Unconditional Console Start Message ---
    print(f"\n>>> Starting Solve from Partitions Run: {run_id} <<<")
    print(f"    Loading partitions from: {os.path.join(args.input_dir, args.partition_run_id)}")
    print(f"    Logs: {full_log_path}")      
    print(f"    Results CSV: {full_csv_path}")    
    if not args.verbose_log: print(f"    Use --verbose_log for detailed console output.")
    print("---")
    logging.info(f"Solve from Partitions Run ID: {run_id}")
    logging.info(f"Partition source: {args.input_dir} / {args.partition_run_id}")
    logging.info(f"Script arguments: {args}")

    # --- Process problem types for 'ALL' --- 
    if len(args.problems) == 1 and args.problems[0].upper() == 'ALL':
        problems_to_iterate = SUPPORTED_PROBLEM_TYPES
        logging.info(f"Processing ALL supported problem types: {problems_to_iterate}")
    else:
        problems_to_iterate = [p.upper() for p in args.problems if p.upper() in SUPPORTED_PROBLEM_TYPES]
        if not problems_to_iterate:
            logging.error("No valid problem types selected from --problems argument. Exiting.")
            print("Error: No valid problem types to process. Check --problems argument.")
            return
        logging.info(f"Processing specified problem types: {problems_to_iterate}")

    # --- Setup Device ---
    use_cuda = not args.no_cuda and torch.cuda.is_available()
    device = torch.device(f'cuda:{args.gpu_id}' if use_cuda else 'cpu')
    logging.info(f"Using device: {device}")

    # --- Determine Resource Availability --- 
    solver_model_available = args.solver_checkpoint is not None
    ortools_available = False
    ortools_solve_vrp_func = None # Keep function pointer if available
    try:
        ortools_solver_module = importlib.import_module('ortools_solver')
        ortools_solve_vrp_func = getattr(ortools_solver_module, 'ortools_solve_vrp')
        ortools_available = True
        logging.info("OR-Tools module found and ortools_solve_vrp function loaded.")
    except (ImportError, AttributeError):
        logging.warning("OR-Tools module (ortools_solver.py) or ortools_solve_vrp function not found. *ORTools methods will be disabled.")

    # --- Filter Requested Methods based on availability (from the fixed 8 methods) --- 
    expanded_methods_from_cli = set()
    for req_method_or_group in args.methods:
        if req_method_or_group == 'AllSolver': expanded_methods_from_cli.update(ALL_SOLVER_METHODS)
        elif req_method_or_group == 'AllORTools': expanded_methods_from_cli.update(ALL_ORTOOLS_METHODS)
        elif req_method_or_group == 'AllPartitionSolver': expanded_methods_from_cli.update(ALL_PARTITION_SOLVER_METHODS)
        elif req_method_or_group == 'AllPartitionORTools': expanded_methods_from_cli.update(ALL_PARTITION_ORTOOLS_METHODS)
        elif req_method_or_group == 'AllPartition': expanded_methods_from_cli.update(ALL_PARTITION_METHODS)
        elif req_method_or_group == 'All_m1': expanded_methods_from_cli.update(ALL_M1_METHODS)
        elif req_method_or_group == 'All_m3': expanded_methods_from_cli.update(ALL_M3_METHODS)
        elif req_method_or_group == 'All_adaptive': expanded_methods_from_cli.update(ALL_ADAPTIVE_METHODS)
        elif req_method_or_group == 'ALL': expanded_methods_from_cli.update(ALL_METHODS) # The main list of 8
        elif req_method_or_group in ALL_METHODS: expanded_methods_from_cli.add(req_method_or_group)
        else: logging.warning(f"Unknown method or group: {req_method_or_group}. It will be ignored.")
    
    methods_to_run = []
    # Iterate in the order of ALL_METHODS to maintain some consistency if groups are used
    for method_name in ALL_METHODS: 
        if method_name not in expanded_methods_from_cli:
            continue # User didn't request this specific method or its group

        if 'Solver' in method_name and not solver_model_available:
            logging.warning(f"Skipping {method_name} (requires --solver_checkpoint).")
            continue
        if 'ORTools' in method_name and not ortools_available:
            logging.warning(f"Skipping {method_name} (OR-Tools not found).")
            continue
        methods_to_run.append(method_name)

    if not methods_to_run:
        logging.error("No methods can be run based on provided arguments and available resources. Exiting.")
        print("Error: No methods to run. Check arguments, model paths, and OR-Tools availability.")
        return
    logging.info(f"Methods to run in this session: {methods_to_run}")

    # --- Load Solver Model (Load ONCE if any Solver method is to be run) ---
    solver_model = None
    solver_params_dict = None # Use a different name to avoid conflict if we pass args obj later
    all_env_classes = {} # To store loaded Env classes {problem_type: EnvClass}

    if any('Solver' in m for m in methods_to_run):
        logging.info("Loading Solver model...")
        try:
            solver_params_dict = DEFAULT_MODEL_PARAMS.copy()
            solver_params_dict['model_type'] = args.solver_model_type
            solver_params_dict['num_experts'] = args.solver_num_experts
            solver_params_dict['device'] = device # Critical: ensure device is in params for model loading
            if args.solver_embedding_dim is not None: solver_params_dict['embedding_dim'] = args.solver_embedding_dim
            if args.solver_ff_hidden_dim is not None: solver_params_dict['ff_hidden_dim'] = args.solver_ff_hidden_dim
            if args.solver_encoder_layer_num is not None: solver_params_dict['encoder_layer_num'] = args.solver_encoder_layer_num
            
            # Problem type for model loading usually comes from checkpoint, or a default like 'Train_ALL'
            # It doesn't need to be the specific problem_type of the instance being solved currently.
            solver_model = load_moe_model(args.solver_checkpoint, device, 
                                            model_type=args.solver_model_type, 
                                            model_params=solver_params_dict)
            if not solver_model: raise ValueError("Solver model loading returned None.")
            solver_model.eval()
            logging.info(f"Solver model loaded from {args.solver_checkpoint}")
        except Exception as e:
            logging.error(f"Failed to load Solver model: {e}", exc_info=True)
            print(f"Error: Failed to load Solver model from {args.solver_checkpoint}. *Solver methods will be disabled.")
            methods_to_run = [m for m in methods_to_run if 'Solver' not in m]
            if not methods_to_run:
                logging.error("No methods remaining after Solver model loading failure. Exiting.")
                print("Error: No methods remaining after Solver model loading failure.")
                return
            logging.info(f"Updated methods to run after solver load fail: {methods_to_run}")

    # --- Prepare CSV Output --- 
    csv_headers = ['problem_type', 'problem_size', 'index_partition', 'index_original', 'partition_run_id', 'method',
                   'score', 'partition_gen_time_seconds', 'subproblem_merge_time_seconds',
                   'solve_time_seconds', 'total_time_seconds', 'num_subproblems'] 
    csvfile = None
    writer = None
    try:
        csvfile = open(full_csv_path, 'w', newline='')
        writer = csv.DictWriter(csvfile, fieldnames=csv_headers)
        writer.writeheader()
        logging.info(f"Opened {full_csv_path} for writing results.")
    except IOError as e:
        logging.error(f"Error opening CSV file {full_csv_path}: {e}. Exiting.")
        print(f"Error: Cannot open output file {full_csv_path}. Check permissions.")
        if csvfile: csvfile.close()
        return

    # --- Main Test Loop --- 
    global_start_time = time.time()
    total_instance_method_evaluations = 0

    # Base path for the specified partition run data (e.g., partition_results/PartitionGen_YYYYMMDD_HHMMSS)
    base_partition_run_path = os.path.join(args.input_dir, args.partition_run_id)
    if not os.path.isdir(base_partition_run_path):
        logging.error(f"Specified partition run directory not found: {base_partition_run_path}. Exiting.")
        print(f"Error: Partition directory {base_partition_run_path} not found.")
        if csvfile: csvfile.close()
        return

    for problem_type_upper in problems_to_iterate:
        logging.info(f"--- Processing Problem Type: {problem_type_upper} ---")
        CurrentEnvClass = None # Specific to the problem_type_upper being processed
        if any('Solver' in m for m in methods_to_run):
            if problem_type_upper not in all_env_classes:
                try:
                    EnvClassList = get_env(problem_type_upper)
                    if not EnvClassList: raise ValueError(f"No EnvClass found for {problem_type_upper}")
                    all_env_classes[problem_type_upper] = EnvClassList[0]
                    logging.info(f"Loaded Env Class for {problem_type_upper}: {all_env_classes[problem_type_upper].__name__}")
                except Exception as e:
                    logging.error(f"Failed to get Env class for {problem_type_upper}: {e}. Solver methods for this type might fail or be skipped.")
                    # Continue, but CurrentEnvClass will be None for this problem_type
            CurrentEnvClass = all_env_classes.get(problem_type_upper)

        for size_to_process in args.sizes:
            logging.info(f"--- Processing Size: {size_to_process} for {problem_type_upper} ---")
            
            # Path to where generate_partitions.py saves instance-specific data for this problem and size
            # e.g., partition_results/PartitionGen_YYYYMMDD_HHMMSS/CVRP/50/
            current_problem_size_partition_path = os.path.join(base_partition_run_path, problem_type_upper, str(size_to_process))
            if not os.path.isdir(current_problem_size_partition_path):
                logging.warning(f"Partition data directory not found for {problem_type_upper} N={size_to_process} at {current_problem_size_partition_path}. Skipping this size.")
                continue

            # --- 获取所有合法的idx目录，按数字顺序排序 ---
            instance_dirs = [d for d in os.listdir(current_problem_size_partition_path) if os.path.isdir(os.path.join(current_problem_size_partition_path, d)) and d.isdigit()]
            instance_indices_sorted = sorted([int(d) for d in instance_dirs])
            if args.num_instances is not None and args.num_instances > 0:
                instance_indices_sorted = instance_indices_sorted[:args.num_instances]
            logging.info(f"Will process {len(instance_indices_sorted)} instances for {problem_type_upper} N={size_to_process}.")

            # Determine effective OR-Tools timelimit for this size (used for DirectORTools and subproblems in PartitionORTools)
            predefined_ort_sizes = np.array(sorted(list(ORTOOLS_TIMELIMIT_MAP.keys()))) # Ensure keys are sorted
            effective_ortools_timelimit_for_size = ORTOOLS_TIMELIMIT_MAP[predefined_ort_sizes[-1]] # Default to largest if N is too big
            
            if size_to_process <= predefined_ort_sizes[0]:
                effective_ortools_timelimit_for_size = ORTOOLS_TIMELIMIT_MAP[predefined_ort_sizes[0]]
            else:
                for i in range(len(predefined_ort_sizes) - 1):
                    if size_to_process > predefined_ort_sizes[i] and size_to_process <= predefined_ort_sizes[i+1]:
                        effective_ortools_timelimit_for_size = ORTOOLS_TIMELIMIT_MAP[predefined_ort_sizes[i+1]]
                        break
            # If size_to_process is larger than all predefined_ort_sizes, it will keep the default from the largest key.

            logging.info(f"For N={size_to_process}, using OR-Tools timelimit: {effective_ortools_timelimit_for_size}s (based on map: {ORTOOLS_TIMELIMIT_MAP})")

            # 按顺序遍历所有idx
            pbar_instances = tqdm(instance_indices_sorted, desc=f"Instances ({problem_type_upper} N={size_to_process})", unit="inst", leave=True)
            for instance_idx_in_partition_run in pbar_instances:
                path_to_this_instance_data = os.path.join(current_problem_size_partition_path, str(instance_idx_in_partition_run))
                logging.debug(f"Processing instance index {instance_idx_in_partition_run} from path: {path_to_this_instance_data}")

                # Load instance_info.json to get original dataset path and true original index
                instance_info_json_path = os.path.join(path_to_this_instance_data, "instance_info.json")
                original_instance_tuple = None
                original_instance_index_in_dataset_from_info = -1 # Default if JSON load fails
                
                # Initialize timing variables for partition and merge times read from logs
                partition_gen_time_from_log = 0.0

                # Try to load instance_info.json and then partition_log.json
                try:
                    if not os.path.exists(instance_info_json_path):
                        raise FileNotFoundError(f"instance_info.json not found in {path_to_this_instance_data}")
                    with open(instance_info_json_path, 'r') as f_info:
                        info_data = json.load(f_info)
                    
                    original_dataset_path_from_info = info_data.get("original_dataset_path")
                    original_instance_index_in_dataset_from_info = info_data.get("original_instance_index")

                    if original_dataset_path_from_info is None or original_instance_index_in_dataset_from_info is None:
                        raise ValueError("instance_info.json is missing original_dataset_path or original_instance_index.")

                    # Now load partition_log.json for this instance to get base partition_gen_time
                    partition_log_path = os.path.join(path_to_this_instance_data, "partition_log.json")
                    if not os.path.exists(partition_log_path):
                        logging.warning(f"partition_log.json not found in {path_to_this_instance_data}. Partition gen time will be 0.")
                    else:
                        with open(partition_log_path, 'r') as f_plog:
                            partition_log_data = json.load(f_plog)
                        partition_gen_time_from_log = partition_log_data.get('partition_time_seconds', 0.0)

                    effective_dataset_path = os.path.join(original_dataset_path_from_info)
                    if not os.path.exists(effective_dataset_path):
                        pass 

                    full_original_dataset = load_dataset(effective_dataset_path, disable_print=True) 
                    if original_instance_index_in_dataset_from_info >= len(full_original_dataset):
                        raise IndexError("original_instance_index out of bounds for the loaded dataset.")
                    original_instance_tuple = full_original_dataset[original_instance_index_in_dataset_from_info]
                
                except FileNotFoundError as e:
                    logging.error(f"Failed to find instance_info.json or dataset file for {problem_type_upper} N={size_to_process} PartitionInstIdx={instance_idx_in_partition_run}: {e}. Skipping this instance.")
                    # Minimal info for CSV on critical load failure
                    writer.writerow({
                        'problem_type': problem_type_upper, 'problem_size': size_to_process,
                        'index_partition': instance_idx_in_partition_run, 'index_original': -1, 
                        'partition_run_id': args.partition_run_id, 'method': 'JSON_OR_DATASET_LOAD_FAIL',
                        'score': 'inf', 'partition_gen_time_seconds': '0.0000', 'subproblem_merge_time_seconds': '0.0000',
                        'solve_time_seconds': '0.0000', 'total_time_seconds': '0.0000', 'num_subproblems': 0
                    })
                    csvfile.flush()
                    continue
                except Exception as e:
                    logging.error(f"Failed to load original instance for {problem_type_upper} N={size_to_process} PartitionInstIdx={instance_idx_in_partition_run} using info from JSON: {e}. Skipping this instance.", exc_info=True)
                    writer.writerow({
                        'problem_type': problem_type_upper, 'problem_size': size_to_process,
                        'index_partition': instance_idx_in_partition_run, 'index_original': original_instance_index_in_dataset_from_info,
                        'partition_run_id': args.partition_run_id, 'method': 'INSTANCE_LOAD_FAIL',
                        'score': 'inf', 'partition_gen_time_seconds': '0.0000', 'subproblem_merge_time_seconds': '0.0000',
                        'solve_time_seconds': '0.0000', 'total_time_seconds': '0.0000', 'num_subproblems': 0
                    })
                    csvfile.flush()
                    continue 
                
                # Time for loading the *original* instance (depot_xy, node_xy etc.) is not directly reported per user request.
                # load_original_instance_time_seconds = time.time() - load_data_time_start 

                # --- Method Loop for this loaded original instance ---
                pbar_methods = tqdm(methods_to_run, desc=f"Methods (Inst {instance_idx_in_partition_run})", unit="method", leave=False)
                for method_to_apply in pbar_methods:
                    pbar_methods.set_description(f"Method: {method_to_apply}")
                    logging.info(f"--- Applying Method: {method_to_apply} for {problem_type_upper} N={size_to_process} PartitionInstIdx={instance_idx_in_partition_run} (OrigIdx {original_instance_index_in_dataset_from_info}) ---")
                    total_instance_method_evaluations += 1
                    
                    # Initialize time components for this method run
                    current_method_partition_gen_time = 0.0
                    current_method_subproblem_merge_time = 0.0
                    current_method_solve_time = 0.0 # From helper result
                    current_method_total_time = 0.0
                    num_subproblems_for_method = 0
                    method_score = float('inf')
                    
                    method_result_dict = None # Stores result from helper functions like run_direct_solver etc.

                    try:
                        if method_to_apply == 'DirectSolver':
                            if solver_model and CurrentEnvClass:
                                # Direct methods have 0 partition/merge time from precomputed logs
                                current_method_partition_gen_time = 0.0
                                current_method_subproblem_merge_time = 0.0
                                method_result_dict = run_direct_solver_single_instance(
                                    original_instance_tuple, problem_type_upper, solver_model, 
                                    CurrentEnvClass, device, args
                                )
                            else: 
                                logging.error("Solver model/Env not available for DirectSolver.")
                                method_result_dict = {'score': float('inf'), 'solve_time_seconds': 0, 'num_subproblems': 0}

                        elif method_to_apply == 'DirectORTools':
                            if ortools_solve_vrp_func:
                                current_method_partition_gen_time = 0.0
                                current_method_subproblem_merge_time = 0.0
                                method_result_dict = run_direct_ortools_single_instance(
                                    original_instance_tuple, problem_type_upper, 
                                    effective_ortools_timelimit_for_size, args, 
                                    ortools_solve_vrp_func
                                )
                            else:
                                logging.error("OR-Tools function not available for DirectORTools.")
                                method_result_dict = {'score': float('inf'), 'solve_time_seconds': 0, 'num_subproblems': 0}
                        
                        elif method_to_apply.startswith('PartitionSolver') or method_to_apply.startswith('PartitionORTools'):
                            current_method_partition_gen_time = partition_gen_time_from_log # Use base time from instance's partition_log.json
                            
                            # Determine suffix for merged_log_*.json (m1, m3, adaptive)
                            method_suffix = None
                            if '_m1' in method_to_apply: method_suffix = 'm1'
                            elif '_m3' in method_to_apply: method_suffix = 'm3'
                            elif '_adaptive' in method_to_apply: method_suffix = 'adaptive'
                            
                            if method_suffix:
                                merged_log_path = os.path.join(path_to_this_instance_data, f"merged_log_{method_suffix}.json")
                                if os.path.exists(merged_log_path):
                                    with open(merged_log_path, 'r') as f_mlog:
                                        merge_log_data = json.load(f_mlog)
                                    current_method_subproblem_merge_time = merge_log_data.get('merge_time_seconds', 0.0)
                                else:
                                    logging.warning(f"Merged log file {merged_log_path} not found. Merge time will be 0.")
                                    current_method_subproblem_merge_time = 0.0
                                    
                                # Load the actual partition data (list of node lists for subproblems)
                                partition_data_pkl_filename = PARTITION_SOLVER_METHOD_TO_FILE.get(method_to_apply) \
                                                            if method_to_apply.startswith('PartitionSolver') else \
                                                            PARTITION_ORTOOLS_METHOD_TO_FILE.get(method_to_apply)
                                
                                if not partition_data_pkl_filename:
                                    logging.error(f"No PKL file mapping found for method {method_to_apply}. Skipping.")
                                    method_result_dict = {'score': float('inf'), 'solve_time_seconds': 0, 'num_subproblems': 0}
                                else:
                                    partition_data_path = os.path.join(path_to_this_instance_data, partition_data_pkl_filename)
                                    if not os.path.exists(partition_data_path):
                                        logging.error(f"Partition data file {partition_data_path} not found for method {method_to_apply}. Skipping.")
                                        method_result_dict = {'score': float('inf'), 'solve_time_seconds': 0, 'num_subproblems': 0}
                                    else:
                                        with open(partition_data_path, 'rb') as f_pkl:
                                            loaded_subproblem_node_lists = pickle.load(f_pkl)
                                        
                                        if method_to_apply.startswith('PartitionSolver'):
                                            if solver_model and CurrentEnvClass:
                                                method_result_dict = run_partition_solver_from_loaded_partitions(
                                                    original_instance_tuple, problem_type_upper, loaded_subproblem_node_lists,
                                                    solver_model, CurrentEnvClass, device, args
                                                )
                                            else: 
                                                logging.error(f"Solver model/Env not available for {method_to_apply}.")
                                                method_result_dict = {'score': float('inf'), 'solve_time_seconds': 0, 'num_subproblems': 0}
                                        
                                        elif method_to_apply.startswith('PartitionORTools'):
                                            if ortools_solve_vrp_func:
                                                # --- 计算所有子问题的填充后尺寸，并据此选择timelimit ---
                                                # 只需一次，所有子问题尺寸一样
                                                # 先生成所有子问题的instance tuple
                                                subproblem_instance_tuples = []
                                                num_original_nodes = 0
                                                try:
                                                    node_xy_idx_orig = VRP_DATA_FORMAT[problem_type_upper].index('node_xy')
                                                    num_original_nodes = len(original_instance_tuple[node_xy_idx_orig])
                                                except (KeyError, IndexError, TypeError) as e:
                                                    logging.error(f"run_partition_ortools: Error accessing original node_xy for count: {e}")
                                                    method_result_dict = {'score': float('inf'), 'solve_time_seconds': 0, 'num_subproblems': 0}
                                                    continue
                                                for node_indices_1based_list in loaded_subproblem_node_lists:
                                                    if not node_indices_1based_list: continue
                                                    valid_indices_for_sub = [idx for idx in node_indices_1based_list if 1 <= idx <= num_original_nodes]
                                                    if not valid_indices_for_sub: continue
                                                    sub_instance = create_subproblem_instance(original_instance_tuple, problem_type_upper, valid_indices_for_sub)
                                                    if sub_instance: subproblem_instance_tuples.append(sub_instance)
                                                
                                                effective_timelimit_for_subproblems = effective_ortools_timelimit_for_size # Default to original problem's timelimit
                                                if subproblem_instance_tuples: # Only if there are subproblems
                                                    # 计算填充后尺寸
                                                    # For PartitionORTools, pad_subproblem_batch's target_pad_size is None, so it uses max nodes in batch
                                                    _padded_sub_batch_for_size_calc, actual_pad_size_for_subs = pad_subproblem_batch(
                                                        subproblem_instance_tuples, problem_type_upper
                                                    )
                                                    subproblem_padded_size = actual_pad_size_for_subs
                                                    
                                                    # 选择最接近的timelimit for subproblems (using the same robust logic)
                                                    predefined_ort_sizes_sub = np.array(sorted(list(ORTOOLS_TIMELIMIT_MAP.keys())))
                                                    effective_timelimit_for_subproblems = ORTOOLS_TIMELIMIT_MAP[predefined_ort_sizes_sub[-1]]
                                                    if subproblem_padded_size <= predefined_ort_sizes_sub[0]:
                                                        effective_timelimit_for_subproblems = ORTOOLS_TIMELIMIT_MAP[predefined_ort_sizes_sub[0]]
                                                    else:
                                                        for i_sub in range(len(predefined_ort_sizes_sub) - 1):
                                                            if subproblem_padded_size > predefined_ort_sizes_sub[i_sub] and subproblem_padded_size <= predefined_ort_sizes_sub[i_sub+1]:
                                                                effective_timelimit_for_subproblems = ORTOOLS_TIMELIMIT_MAP[predefined_ort_sizes_sub[i_sub+1]]
                                                                break
                                                    logging.info(f"PartitionORTools: Subproblem padded size N_sub_pad={subproblem_padded_size}, using OR-Tools timelimit: {effective_timelimit_for_subproblems}s for subproblems.")
                                                else:
                                                    logging.warning("PartitionORTools: No valid subproblem instances, using default OR-Tools timelimit for subproblems.")

                                                # 调用
                                                method_result_dict = run_partition_ortools_from_loaded_partitions(
                                                    original_instance_tuple, problem_type_upper, loaded_subproblem_node_lists,
                                                    effective_timelimit_for_subproblems, args, ortools_solve_vrp_func
                                                )
                                            else:
                                                logging.error(f"OR-Tools function not available for {method_to_apply}.")
                                                method_result_dict = {'score': float('inf'), 'solve_time_seconds': 0, 'num_subproblems': 0}
                            else: # method_suffix could not be determined
                                logging.error(f"Could not determine method suffix for {method_to_apply} to load merge log. Skipping.")
                                method_result_dict = {'score': float('inf'), 'solve_time_seconds': 0, 'num_subproblems': 0}
                        else:
                            logging.warning(f"Method {method_to_apply} not recognized for specific time handling or execution. Skipping.")
                            method_result_dict = {'score': float('inf'), 'solve_time_seconds': 0, 'num_subproblems': 0}

                        # Extract results from the helper function's dictionary
                        if method_result_dict:
                            method_score = method_result_dict.get('score', float('inf'))
                            current_method_solve_time = method_result_dict.get('solve_time_seconds', 0.0)
                            num_subproblems_for_method = method_result_dict.get('num_subproblems', 0)
                        else: # Should have been initialized if a known method failed
                            method_score = float('inf')
                            current_method_solve_time = 0.0
                            num_subproblems_for_method = 0
                            logging.error(f"method_result_dict was None after trying to run {method_to_apply}")

                    except Exception as method_err:
                        logging.error(f"Error running method {method_to_apply} for instance {instance_idx_in_partition_run}: {method_err}", exc_info=True)
                        method_score = float('inf')
                        # Times will remain as initialized (likely 0)
                        num_subproblems_for_method = 0 # Ensure it's set for CSV writing
                    
                    # Calculate total time based on the new components
                    # For PartitionORTools_* methods, use adjusted total_time_seconds
                    if method_to_apply.startswith('PartitionORTools'):
                        num_subproblems = num_subproblems_for_method if num_subproblems_for_method > 0 else 1
                        workers = getattr(args, 'subproblem_workers', 16)
                        batches = math.ceil(num_subproblems / workers) if workers > 0 else 1
                        if batches < 1:
                            batches = 1
                        current_method_total_time = (
                            current_method_partition_gen_time +
                            current_method_subproblem_merge_time +
                            (current_method_solve_time / batches)
                        )
                    else:
                        current_method_total_time = current_method_partition_gen_time + current_method_subproblem_merge_time + current_method_solve_time

                    # --- Memory cleanup after each method evaluation ---
                    if use_cuda:
                        torch.cuda.empty_cache()
                    gc.collect()
                    logging.debug(f"Memory cleanup after method {method_to_apply} for instance {instance_idx_in_partition_run}, {problem_type_upper} N={size_to_process}")
                
                    # --- Write result to CSV (MOVED INSIDE METHOD LOOP FOR REAL-TIME WRITING) --- 
                    writer.writerow({
                        'problem_type': problem_type_upper,
                        'problem_size': size_to_process,
                        'index_partition': instance_idx_in_partition_run, 
                        'index_original': original_instance_index_in_dataset_from_info,
                        'partition_run_id': args.partition_run_id,
                        'method': method_to_apply,
                        'score': f"{method_score:.4f}" if method_score != float('inf') else 'inf',
                        'partition_gen_time_seconds': f"{current_method_partition_gen_time:.4f}",
                        'subproblem_merge_time_seconds': f"{current_method_subproblem_merge_time:.4f}",
                        'solve_time_seconds': f"{current_method_solve_time:.4f}",
                        'total_time_seconds': f"{current_method_total_time:.4f}",
                        'num_subproblems': num_subproblems_for_method
                    })
                    csvfile.flush() # Ensure data is written to disk immediately

                pbar_methods.close() # Close method progress bar
                # --- End Method Loop ---
            
            pbar_instances.close() # Close instance progress bar
            # --- End Instance Loop (from partition data) ---
            logging.info(f"--- Finished processing Size: {size_to_process} for {problem_type_upper} ---")
        # End size loop
        logging.info(f"--- Finished processing Problem Type: {problem_type_upper} ---")
    # End problem_type loop

    global_end_time = time.time()
    logging.info(f"Solve from partitions run finished. Processed {total_instance_method_evaluations} total instance-method evaluations.")
    logging.info(f"Total time: {global_end_time - global_start_time:.2f} seconds.")
    if csvfile: 
        try: csvfile.close() 
        except: pass
        logging.info(f"Results CSV closed: {full_csv_path}")
    print("---")
    print(f">>> Finished Solve from Partitions Run: {run_id} <<<")
    print(f"    Results saved to: {full_csv_path}")
    print(f"    Detailed logs in: {full_log_path}")

# --------------------------------------------------------------------------
# 6. Script Execution Guard
# --------------------------------------------------------------------------
if __name__ == "__main__":
    import multiprocessing as mp
    if mp.get_start_method(allow_none=True) != 'spawn':
        try:
            mp.set_start_method('spawn', force=True)
            print(f"INFO: Multiprocessing start method successfully set to 'spawn'.") 
        except RuntimeError as e:
            print(f"WARNING: Could not set multiprocessing start method to 'spawn': {e}. Ensure this is at script entry if issues persist with CUDA & ProcessPoolExecutor.")
    main_solver() 
